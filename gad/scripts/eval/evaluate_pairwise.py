#!/usr/bin/env python3
"""
Pairwise evaluation using LLM-as-a-Judge.

This script evaluates model outputs by comparing student vs teacher responses
using GPT-4o (or other LLMs) as an impartial judge.

Supports both JSONL and Parquet input formats (auto-detected by extension).

Reuses existing codebase components:
- deepscaler/utils.py -> call_oai_rm_llm() for OpenAI API calls
- deepscaler/rewards/judge_extractor.py -> extract_judge() for parsing results
- verl/verl/utils/dataset/prompt_templates.py -> MYPROMPT2 judge template

Usage:
    # With JSONL files
    python evaluate_pairwise.py \
        --student /path/to/student_results.jsonl \
        --teacher /path/to/teacher_results.jsonl \
        --output /path/to/eval_results.json \
        --judge-model gpt-4o \
        --workers 4

    # With Parquet files (recommended)
    python evaluate_pairwise.py \
        --student /path/to/student_results.parquet \
        --teacher /path/to/teacher_data.parquet \
        --output /path/to/eval_results.json \
        --judge-model gpt-4o
"""
import argparse
import json
import os
import random
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Dict, List, Optional
from tqdm import tqdm

# Add parent directory to path to import existing modules
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from deepscaler.utils import call_oai_rm_llm
from deepscaler.rewards.judge_extractor import extract_judge
from deepscaler.globals import THOUGHT_DELIMITER_END


# Judge prompt template (adapted from prompt_templates.py - MYPROMPT2)
JUDGE_SYSTEM_PROMPT = """You are a helpful assistant in evaluating the quality of the responses for a given instruction. Your goal is to select the best response for the given instruction.
Select Assistant 1 or Assistant 2, that is better for the given instruction. The two responses are generated by two different AI assistants respectively.
Do NOT say both / neither are good.

Here are some rules of the evaluation:
(1) If the instruction does not contain harmful content, you should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) If the instruction contains harmful content, prioritize the harmlessness and safety of the response.
(3) Responses should NOT contain more/less than what the instruction asks for, as such responses do NOT precisely execute the instruction.
(4) You should avoid any potential bias and your judgment should be as objective as possible. Here are some potential sources of bias:
- The order in which the responses were presented should NOT affect your judgment, as Response A and Response B are **equally likely** to be the better.
- The length of the responses should NOT affect your judgement, as a longer response does not necessarily correspond to a better response. When making your decision, evaluate if the response length is appropriate for the given instruction.
(5) Your output should only consist of '\\boxed{Assistant 1}' if assistant 1 is better, or '\\boxed{Assistant 2}' if assistant 2 is better. Omit any other output."""

JUDGE_USER_TEMPLATE = """## Query

{question}

## Assistant responses

### Assistant 1

{answer1}


### Assistant 2

{answer2}


## Analysis

Let's analyze this step by step and decide which assistant is better, and then answer \\boxed{{Assistant 1}} or \\boxed{{Assistant 2}}."""


def remove_think_tags(text: str) -> str:
    """Remove <think>...</think> blocks from response text."""
    if THOUGHT_DELIMITER_END in text:
        parts = text.split(THOUGHT_DELIMITER_END)
        if len(parts) >= 2:
            return parts[-1].strip()
    return text


def judge_pair(
    query: str,
    response_a: str,
    response_b: str,
    model_id: str = "gpt-4o",
    shuffle: bool = True,
    seed: Optional[int] = None
) -> Dict:
    """
    Judge a pair of responses using LLM-as-a-Judge.

    Args:
        query: The user query/question
        response_a: First response (typically student)
        response_b: Second response (typically teacher)
        model_id: Model to use as judge
        shuffle: Whether to randomly shuffle response order (anti-position-bias)
        seed: Random seed for reproducibility

    Returns:
        Dict with winner, original positions, raw response, and validity flag
    """
    # Clean responses by removing think tags
    response_a_clean = remove_think_tags(response_a)
    response_b_clean = remove_think_tags(response_b)

    # Shuffle to mitigate position bias
    rng = random.Random(seed) if seed is not None else random.Random()
    if shuffle and rng.random() < 0.5:
        answer1, answer2 = response_b_clean, response_a_clean
        swapped = True
    else:
        answer1, answer2 = response_a_clean, response_b_clean
        swapped = False

    prompt = JUDGE_USER_TEMPLATE.format(
        question=query,
        answer1=answer1,
        answer2=answer2
    )

    try:
        judge_response = call_oai_rm_llm(
            prompt=prompt,
            system_prompt=JUDGE_SYSTEM_PROMPT,
            model_id=model_id,
            temperature=0.0,
            n=1
        )
    except Exception as e:
        return {
            "winner": "error",
            "swapped": swapped,
            "raw_response": str(e),
            "valid": False,
            "error": str(e)
        }

    # Handle empty response
    if not judge_response:
        return {
            "winner": "invalid",
            "swapped": swapped,
            "raw_response": "",
            "valid": False
        }

    invalid_metric = {"n_judge": 0, "n_invalid_judge": 0}
    result = extract_judge(judge_response, invalid_metric)

    # Map back to original positions (A=student, B=teacher)
    if swapped:
        if result == 1:  # Assistant 1 won, but that was teacher
            winner = "teacher"
        elif result == 2:  # Assistant 2 won, but that was student
            winner = "student"
        else:
            winner = "invalid"
    else:
        if result == 1:  # Assistant 1 won, that was student
            winner = "student"
        elif result == 2:  # Assistant 2 won, that was teacher
            winner = "teacher"
        else:
            winner = "invalid"

    return {
        "winner": winner,
        "swapped": swapped,
        "raw_response": judge_response,
        "valid": result != -1
    }


def load_results(filepath: str) -> List[Dict]:
    """Load results from JSONL or Parquet file (auto-detected by extension)."""
    if filepath.endswith('.parquet'):
        import pandas as pd
        df = pd.read_parquet(filepath)
        return df.to_dict('records')
    else:
        # JSONL format
        results = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    results.append(json.loads(line))
        return results


def extract_query_from_conversation(content) -> str:
    """Extract query from conversation format (list of messages)."""
    if isinstance(content, list):
        # Extract all user messages and join them
        user_msgs = []
        for msg in content:
            if isinstance(msg, dict) and msg.get('role') == 'user':
                user_msgs.append(msg.get('content', ''))
        if user_msgs:
            return '\n'.join(user_msgs)
        # Fallback: return string representation
        return str(content)
    return str(content) if content else None


def load_generation_results(filepath: str) -> List[Dict]:
    """Load generation results from JSONL or Parquet file."""
    return load_results(filepath)


def run_evaluation(
    student_results_file: str,
    teacher_results_file: str,
    output_file: str,
    judge_model: str = "gpt-4o",
    max_workers: int = 4,
    limit: Optional[int] = None,
    shuffle: bool = True,
    seed: int = 42,
    query_key: str = "prompt",
    response_key: str = "response",
    resume: bool = False
):
    """
    Run pairwise evaluation comparing student vs teacher responses.

    Args:
        student_results_file: Path to student generation results JSONL
        teacher_results_file: Path to teacher generation results JSONL
        output_file: Path for output JSON file
        judge_model: Model to use as judge (e.g., gpt-4o, gpt-4o-mini)
        max_workers: Number of parallel API workers
        limit: Limit number of comparisons (for testing)
        shuffle: Whether to shuffle response order
        seed: Random seed for reproducibility
        query_key: Key for query/prompt in JSONL
        response_key: Key for response in JSONL
        resume: Whether to resume from existing output file
    """
    print(f"Loading student results from {student_results_file}")
    student_results = load_generation_results(student_results_file)
    print(f"  Loaded {len(student_results)} student results")

    print(f"Loading teacher results from {teacher_results_file}")
    teacher_results = load_generation_results(teacher_results_file)
    print(f"  Loaded {len(teacher_results)} teacher results")

    # Build lookup by query
    def get_query(item):
        # Try multiple possible keys for the query
        for key in [query_key, "prompt", "query", "question", "content"]:
            if key in item:
                value = item[key]
                # Handle conversation format (list of messages)
                if isinstance(value, list):
                    return extract_query_from_conversation(value)
                return value
        return None

    def get_response(item):
        # Try multiple possible keys for the response
        for key in [response_key, "response", "output", "answer", "completion", "teacher_response"]:
            if key in item:
                return item[key]
        return None

    teacher_by_query = {}
    for r in teacher_results:
        q = get_query(r)
        if q:
            teacher_by_query[q] = get_response(r)

    # Build pairs
    pairs = []
    for student in student_results:
        query = get_query(student)
        if query and query in teacher_by_query:
            pairs.append({
                "query": query,
                "student_response": get_response(student),
                "teacher_response": teacher_by_query[query]
            })

    print(f"Matched {len(pairs)} query pairs")

    if limit:
        pairs = pairs[:limit]
        print(f"Limited to {limit} pairs")

    # Resume from existing results if requested
    completed_queries = set()
    existing_results = []
    if resume and os.path.exists(output_file):
        try:
            with open(output_file, 'r') as f:
                existing_data = json.load(f)
                existing_results = existing_data.get("results", [])
                completed_queries = {r["query"] for r in existing_results}
                print(f"Resuming from {len(completed_queries)} completed evaluations")
        except Exception as e:
            print(f"Could not resume from existing file: {e}")

    # Filter out already completed pairs
    pairs_to_evaluate = [p for p in pairs if p["query"] not in completed_queries]
    print(f"Evaluating {len(pairs_to_evaluate)} new pairs")

    if not pairs_to_evaluate:
        print("No new pairs to evaluate")
        return

    results = list(existing_results)
    stats = {
        "student_wins": sum(1 for r in existing_results if r.get("winner") == "student"),
        "teacher_wins": sum(1 for r in existing_results if r.get("winner") == "teacher"),
        "invalid": sum(1 for r in existing_results if r.get("winner") in ["invalid", "error"]),
        "total": len(existing_results)
    }

    # Create thread-safe progress bar
    pbar = tqdm(total=len(pairs_to_evaluate), desc="Judging pairs")

    def evaluate_pair(pair_with_idx):
        idx, pair = pair_with_idx
        result = judge_pair(
            pair["query"],
            pair["student_response"],
            pair["teacher_response"],
            model_id=judge_model,
            shuffle=shuffle,
            seed=seed + idx if seed else None
        )
        return pair, result

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(evaluate_pair, (i, p)): i
            for i, p in enumerate(pairs_to_evaluate)
        }

        for future in as_completed(futures):
            try:
                pair, result = future.result()

                results.append({
                    "query": pair["query"],
                    "student_response": pair["student_response"],
                    "teacher_response": pair["teacher_response"],
                    **result
                })

                stats["total"] += 1
                if result["winner"] == "student":
                    stats["student_wins"] += 1
                elif result["winner"] == "teacher":
                    stats["teacher_wins"] += 1
                else:
                    stats["invalid"] += 1

            except Exception as e:
                print(f"Error evaluating pair: {e}")
                stats["total"] += 1
                stats["invalid"] += 1

            pbar.update(1)

            # Save intermediate results every 50 evaluations
            if stats["total"] % 50 == 0:
                save_results(output_file, stats, results)

    pbar.close()

    # Final save
    save_results(output_file, stats, results)
    print_summary(stats, output_file)


def save_results(output_file: str, stats: Dict, results: List):
    """Save evaluation results to JSON file."""
    valid_total = stats["total"] - stats["invalid"]
    win_rate = stats["student_wins"] / valid_total if valid_total > 0 else 0

    output_data = {
        "metadata": {
            "timestamp": datetime.now().isoformat(),
            "total_comparisons": stats["total"],
            "valid_comparisons": valid_total,
            "invalid_comparisons": stats["invalid"],
        },
        "stats": stats,
        "win_rate": win_rate,
        "results": results
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)


def print_summary(stats: Dict, output_file: str):
    """Print evaluation summary."""
    valid_total = stats["total"] - stats["invalid"]
    win_rate = stats["student_wins"] / valid_total if valid_total > 0 else 0

    print("\n" + "="*60)
    print("EVALUATION SUMMARY")
    print("="*60)
    print(f"Results saved to: {output_file}")
    print(f"Total comparisons: {stats['total']}")
    print(f"Valid comparisons: {valid_total}")
    print(f"Invalid judgments: {stats['invalid']} ({stats['invalid']/stats['total']*100:.1f}%)")
    print("-"*60)
    print(f"Student wins: {stats['student_wins']} ({stats['student_wins']/valid_total*100:.1f}%)" if valid_total else "Student wins: N/A")
    print(f"Teacher wins: {stats['teacher_wins']} ({stats['teacher_wins']/valid_total*100:.1f}%)" if valid_total else "Teacher wins: N/A")
    print("-"*60)
    print(f"STUDENT WIN RATE: {win_rate:.2%}")
    print("="*60)


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate model outputs using LLM-as-a-Judge pairwise comparison",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Basic usage
    python evaluate_pairwise.py \\
        --student student_results.jsonl \\
        --teacher teacher_results.jsonl \\
        --output eval_results.json

    # Use cheaper model for testing
    python evaluate_pairwise.py \\
        --student student_results.jsonl \\
        --teacher teacher_results.jsonl \\
        --output eval_results.json \\
        --judge-model gpt-4o-mini \\
        --limit 100

    # Resume interrupted evaluation
    python evaluate_pairwise.py \\
        --student student_results.jsonl \\
        --teacher teacher_results.jsonl \\
        --output eval_results.json \\
        --resume
        """
    )
    parser.add_argument(
        "--student", required=True,
        help="Path to student generation results (JSONL or Parquet format)"
    )
    parser.add_argument(
        "--teacher", required=True,
        help="Path to teacher generation results (JSONL or Parquet format)"
    )
    parser.add_argument(
        "--output", required=True,
        help="Output file for evaluation results (JSON format)"
    )
    parser.add_argument(
        "--judge-model", default="gpt-4o",
        help="Model to use as judge (default: gpt-4o)"
    )
    parser.add_argument(
        "--workers", type=int, default=4,
        help="Number of parallel workers (default: 4)"
    )
    parser.add_argument(
        "--limit", type=int, default=None,
        help="Limit number of comparisons (for testing)"
    )
    parser.add_argument(
        "--no-shuffle", action="store_true",
        help="Disable response order shuffling (not recommended)"
    )
    parser.add_argument(
        "--seed", type=int, default=42,
        help="Random seed for reproducibility (default: 42)"
    )
    parser.add_argument(
        "--query-key", default="prompt",
        help="Key for query/prompt in JSONL (default: prompt)"
    )
    parser.add_argument(
        "--response-key", default="response",
        help="Key for response in JSONL (default: response)"
    )
    parser.add_argument(
        "--resume", action="store_true",
        help="Resume from existing output file"
    )

    args = parser.parse_args()

    # Validate inputs
    if not os.path.exists(args.student):
        print(f"Error: Student results file not found: {args.student}")
        sys.exit(1)
    if not os.path.exists(args.teacher):
        print(f"Error: Teacher results file not found: {args.teacher}")
        sys.exit(1)

    # Check for OpenAI API key
    if not os.environ.get("OPENAI_API_KEY"):
        print("Warning: OPENAI_API_KEY environment variable not set")
        print("Please set it: export OPENAI_API_KEY=your_key")

    run_evaluation(
        student_results_file=args.student,
        teacher_results_file=args.teacher,
        output_file=args.output,
        judge_model=args.judge_model,
        max_workers=args.workers,
        limit=args.limit,
        shuffle=not args.no_shuffle,
        seed=args.seed,
        query_key=args.query_key,
        response_key=args.response_key,
        resume=args.resume
    )


if __name__ == "__main__":
    main()
