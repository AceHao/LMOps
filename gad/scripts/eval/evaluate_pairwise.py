#!/usr/bin/env python3
"""
Pairwise evaluation using LLM-as-a-Judge.

Compares student vs teacher responses using GPT-4o (or other LLMs) as judge.

Usage:
    python evaluate_pairwise.py \
        --results /path/to/generation_results.jsonl \
        --output /path/to/eval_results.json \
        --judge-model gpt-4o \
        --workers 4

Expected input format (JSONL):
    {"input": "...", "output": "student response", "teacher_output": "teacher response"}
"""
import argparse
import json
import os
import random
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Dict, List, Optional
from tqdm import tqdm

# Add parent directory to path to import existing modules
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from deepscaler.utils import call_oai_rm_llm
from deepscaler.rewards.judge_extractor import extract_judge
from deepscaler.globals import THOUGHT_DELIMITER_END


JUDGE_SYSTEM_PROMPT = """You are a helpful assistant in evaluating the quality of the responses for a given instruction. Your goal is to select the best response for the given instruction.
Select Assistant 1 or Assistant 2, that is better for the given instruction. The two responses are generated by two different AI assistants respectively.
Do NOT say both / neither are good.

Here are some rules of the evaluation:
(1) If the instruction does not contain harmful content, you should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) If the instruction contains harmful content, prioritize the harmlessness and safety of the response.
(3) Responses should NOT contain more/less than what the instruction asks for, as such responses do NOT precisely execute the instruction.
(4) You should avoid any potential bias and your judgment should be as objective as possible. Here are some potential sources of bias:
- The order in which the responses were presented should NOT affect your judgment, as Response A and Response B are **equally likely** to be the better.
- The length of the responses should NOT affect your judgement, as a longer response does not necessarily correspond to a better response. When making your decision, evaluate if the response length is appropriate for the given instruction.
(5) Your output should only consist of '\\boxed{Assistant 1}' if assistant 1 is better, or '\\boxed{Assistant 2}' if assistant 2 is better. Omit any other output."""

JUDGE_USER_TEMPLATE = """## Query

{question}

## Assistant responses

### Assistant 1

{answer1}


### Assistant 2

{answer2}


## Analysis

Let's analyze this step by step and decide which assistant is better, and then answer \\boxed{{Assistant 1}} or \\boxed{{Assistant 2}}."""


def remove_think_tags(text: Optional[str]) -> str:
    """Remove <think>...</think> blocks from response text."""
    if text is None:
        return ""
    if THOUGHT_DELIMITER_END in text:
        parts = text.split(THOUGHT_DELIMITER_END)
        if len(parts) >= 2:
            return parts[-1].strip()
    return text


def judge_pair(
    query: str,
    student_response: str,
    teacher_response: str,
    model_id: str = "gpt-4o",
    shuffle: bool = True,
    seed: Optional[int] = None
) -> Dict:
    """Judge a pair of responses using LLM-as-a-Judge."""
    student_clean = remove_think_tags(student_response)
    teacher_clean = remove_think_tags(teacher_response)

    # Shuffle to mitigate position bias
    rng = random.Random(seed) if seed is not None else random.Random()
    if shuffle and rng.random() < 0.5:
        answer1, answer2 = teacher_clean, student_clean
        swapped = True
    else:
        answer1, answer2 = student_clean, teacher_clean
        swapped = False

    prompt = JUDGE_USER_TEMPLATE.format(
        question=query,
        answer1=answer1,
        answer2=answer2
    )

    try:
        judge_response = call_oai_rm_llm(
            prompt=prompt,
            system_prompt=JUDGE_SYSTEM_PROMPT,
            model_id=model_id,
            temperature=0.0,
            n=1
        )
    except Exception as e:
        return {"winner": "error", "swapped": swapped, "raw_response": str(e), "valid": False, "error": str(e)}

    if not judge_response:
        return {"winner": "invalid", "swapped": swapped, "raw_response": "", "valid": False}

    invalid_metric = {"n_judge": 0, "n_invalid_judge": 0}
    result = extract_judge(judge_response, invalid_metric)

    # Map back to original positions
    if swapped:
        winner = "teacher" if result == 1 else ("student" if result == 2 else "invalid")
    else:
        winner = "student" if result == 1 else ("teacher" if result == 2 else "invalid")

    return {"winner": winner, "swapped": swapped, "raw_response": judge_response, "valid": result != -1}


def load_results(filepath: str) -> List[Dict]:
    """Load results from JSONL or Parquet file."""
    if filepath.endswith('.parquet'):
        import pandas as pd
        df = pd.read_parquet(filepath)
        return df.to_dict('records')
    else:
        results = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    results.append(json.loads(line))
        return results


def run_evaluation(
    results_file: str,
    output_file: str,
    judge_model: str = "gpt-4o",
    max_workers: int = 4,
    limit: Optional[int] = None,
    shuffle: bool = True,
    seed: int = 42,
    resume: bool = False
):
    """Run pairwise evaluation on generation results."""
    print(f"Loading results from {results_file}")
    data = load_results(results_file)
    print(f"  Loaded {len(data)} records")

    # Build pairs from the single file
    pairs = []
    skipped = 0
    for item in data:
        query = item.get("input", "")
        student_response = item.get("output")
        teacher_response = item.get("teacher_output")

        if student_response is None or teacher_response is None:
            skipped += 1
            continue

        pairs.append({
            "query": query,
            "student_response": student_response,
            "teacher_response": teacher_response
        })

    if skipped > 0:
        print(f"  Skipped {skipped} records (missing output or teacher_output)")

    print(f"  Found {len(pairs)} valid pairs")

    if limit:
        pairs = pairs[:limit]
        print(f"  Limited to {limit} pairs")

    # Resume from existing results if requested
    completed_queries = set()
    existing_results = []
    if resume and os.path.exists(output_file):
        try:
            with open(output_file, 'r') as f:
                existing_data = json.load(f)
                existing_results = existing_data.get("results", [])
                completed_queries = {r["query"] for r in existing_results}
                print(f"  Resuming from {len(completed_queries)} completed evaluations")
        except Exception as e:
            print(f"  Could not resume: {e}")

    pairs_to_evaluate = [p for p in pairs if p["query"] not in completed_queries]
    print(f"  Evaluating {len(pairs_to_evaluate)} new pairs")

    if not pairs_to_evaluate:
        print("No new pairs to evaluate")
        return

    results = list(existing_results)
    stats = {
        "student_wins": sum(1 for r in existing_results if r.get("winner") == "student"),
        "teacher_wins": sum(1 for r in existing_results if r.get("winner") == "teacher"),
        "invalid": sum(1 for r in existing_results if r.get("winner") in ["invalid", "error"]),
        "total": len(existing_results)
    }

    pbar = tqdm(total=len(pairs_to_evaluate), desc="Judging pairs")

    def evaluate_pair(pair_with_idx):
        idx, pair = pair_with_idx
        result = judge_pair(
            pair["query"],
            pair["student_response"],
            pair["teacher_response"],
            model_id=judge_model,
            shuffle=shuffle,
            seed=seed + idx if seed else None
        )
        return pair, result

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(evaluate_pair, (i, p)): i for i, p in enumerate(pairs_to_evaluate)}

        for future in as_completed(futures):
            try:
                pair, result = future.result()
                results.append({
                    "query": pair["query"],
                    "student_response": pair["student_response"],
                    "teacher_response": pair["teacher_response"],
                    **result
                })
                stats["total"] += 1
                if result["winner"] == "student":
                    stats["student_wins"] += 1
                elif result["winner"] == "teacher":
                    stats["teacher_wins"] += 1
                else:
                    stats["invalid"] += 1
            except Exception as e:
                print(f"Error: {e}")
                stats["total"] += 1
                stats["invalid"] += 1

            pbar.update(1)
            if stats["total"] % 50 == 0:
                save_results(output_file, stats, results)

    pbar.close()
    save_results(output_file, stats, results)
    print_summary(stats, output_file)


def save_results(output_file: str, stats: Dict, results: List):
    """Save evaluation results to JSON file."""
    valid_total = stats["total"] - stats["invalid"]
    output_data = {
        "metadata": {
            "timestamp": datetime.now().isoformat(),
            "total_comparisons": stats["total"],
            "valid_comparisons": valid_total,
            "invalid_comparisons": stats["invalid"],
        },
        "stats": stats,
        "win_rate": stats["student_wins"] / valid_total if valid_total > 0 else 0,
        "results": results
    }
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)


def print_summary(stats: Dict, output_file: str):
    """Print evaluation summary."""
    valid_total = stats["total"] - stats["invalid"]
    win_rate = stats["student_wins"] / valid_total if valid_total > 0 else 0

    print("\n" + "="*60)
    print("EVALUATION SUMMARY")
    print("="*60)
    print(f"Results saved to: {output_file}")
    print(f"Total comparisons: {stats['total']}")
    print(f"Valid comparisons: {valid_total}")
    if stats['total'] > 0:
        print(f"Invalid judgments: {stats['invalid']} ({stats['invalid']/stats['total']*100:.1f}%)")
    print("-"*60)
    if valid_total > 0:
        print(f"Student wins: {stats['student_wins']} ({stats['student_wins']/valid_total*100:.1f}%)")
        print(f"Teacher wins: {stats['teacher_wins']} ({stats['teacher_wins']/valid_total*100:.1f}%)")
    print("-"*60)
    print(f"STUDENT WIN RATE: {win_rate:.2%}")
    print("="*60)


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate model outputs using LLM-as-a-Judge pairwise comparison",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python evaluate_pairwise.py \\
        --results generation_results.jsonl \\
        --output eval_results.json

    python evaluate_pairwise.py \\
        --results generation_results.jsonl \\
        --output eval_results.json \\
        --judge-model gpt-4o-mini \\
        --limit 100
        """
    )
    parser.add_argument("--results", required=True, help="Path to generation results (JSONL with input, output, teacher_output)")
    parser.add_argument("--output", required=True, help="Output file for evaluation results (JSON)")
    parser.add_argument("--judge-model", default="gpt-4o", help="Model to use as judge (default: gpt-4o)")
    parser.add_argument("--workers", type=int, default=4, help="Number of parallel workers (default: 4)")
    parser.add_argument("--limit", type=int, default=None, help="Limit number of comparisons")
    parser.add_argument("--no-shuffle", action="store_true", help="Disable response order shuffling")
    parser.add_argument("--seed", type=int, default=42, help="Random seed (default: 42)")
    parser.add_argument("--resume", action="store_true", help="Resume from existing output file")

    args = parser.parse_args()

    if not os.path.exists(args.results):
        print(f"Error: Results file not found: {args.results}")
        sys.exit(1)

    if not os.environ.get("OPENAI_API_KEY"):
        print("Warning: OPENAI_API_KEY environment variable not set")

    run_evaluation(
        results_file=args.results,
        output_file=args.output,
        judge_model=args.judge_model,
        max_workers=args.workers,
        limit=args.limit,
        shuffle=not args.no_shuffle,
        seed=args.seed,
        resume=args.resume
    )


if __name__ == "__main__":
    main()
